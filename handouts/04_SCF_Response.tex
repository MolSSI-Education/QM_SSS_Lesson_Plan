%\documentclass[12pt]{article}
\documentclass[aip,jcp,preprint,superscriptaddress,floatfix]{revtex4-1}
\usepackage{url,graphicx,tabularx,array,geometry,amsmath,listings}
\setlength{\parskip}{2ex} %--skip lines between paragraphs
\setlength{\parindent}{20pt} %--don't indent paragraphs

\setlength{\headheight}{-50pt}
\setlength{\textheight}{700pt}
\setlength{\textwidth}{500pt}
\setlength{\oddsidemargin}{-10pt}
\setlength{\footskip}{50pt}
\usepackage{graphicx}% Include figure files
\usepackage{bm}
\graphicspath{{./Figures/}}

%-- Commands for header
\renewcommand{\title}[1]{\textbf{\large{#1}}\\}
\renewcommand{\line}{\begin{tabularx}{\textwidth}{X>{\raggedleft}X}\hline\\\end{tabularx}\\[-0.5cm]}
\newcommand{\leftright}[2]{\begin{tabularx}{\textwidth}{X>{\raggedleft}X}#1%
& #2\\\end{tabularx}\\[-1cm]}

%\linespread{2} %-- Uncomment for Double Space
\begin{document}

\title{\center{SCF Response}}
\rule{\textwidth}{1pt}
\leftright{The Molecular Sciences Software Institute}{Daniel G. A. Smith} %-- left and right positions in the header

\bigskip
\section{Second-order orbital optimization}
To begin our response discussion at the SCF level we will first discuss solving the SCF equations with full Newton steps rather than diagonalization.
Where diagonalization builds completely new orbitals at every iteration; this Newton scheme will construct new orbitals by rotating the orbitals of the previous iteration.
These rotations will be formed by fundamental linear algebra principles as their gradient and Hessian is well defined.
This is often called second-order optimization and, if done exactly, the convergence will be quadratic with respect to the MO gradient\cite{Helgaker:2000ug}, e.g., ${\rm grad}_{pq}^{n+1} \approx ({{\rm grad}_{pq}^{n}}) ^2$.
As a consequence of the quadratic convergence, the second-order procedure can only be started once all elements of the gradient in the MO basis are less than one, else the error will increase. 

The overall goal is to find an orbital rotation matrix, ${\bm \kappa}$, that satisfies the following set of linear equations:

\begin{eqnarray}
\label{somcscf_master}
{\rm\bf E}^{(2)} {\bm \kappa} = -{\rm\bf E}^{(1)}
\end{eqnarray}

where {\bf E}$^{(2)}$ is our orbital Hessian and {\bf E}$^{(1)}$ is the orbital gradient. 
The ${\bm \kappa}$ matrix is a antisymmetric matrix of non-redundant rotations that describes the unitary transformation {\bf U}
\begin{align}
 {\rm\bf U} &= e^{\bm \kappa}\\
 {\rm\bf C}_{n+1} &= {\rm\bf C}_{n}{\rm\bf U}
\end{align}
Rotations within a given space (inactive or virtual for SCF) are redundant: the only non-redundant orbital rotations for SCF are the inactive-virtual ones.

The orbital gradient for RHF gradient can be written as,
\begin{eqnarray}
{\rm E}^{(1)}_{pq} = F_{pq}
\end{eqnarray}

where $F$ is our generalized Fock matrix\cite{Helgaker:2000ug} in MO indices,

\begin{align}
{F}_{pq} &= h_{pq} + 2 g_{pqrs} D_{rs} - g_{prqs} D_{rs}
\end{align}

To derive our orbital-response Hessian we can introduce the one-index and two-index transformed integrals
\begin{align}
h^{\kappa}_{pq} &=  (\kappa_{po}h_{oq} + \kappa_{qo}h_{po})\\
g^{\kappa}_{pqrs} &= (\kappa_{po}g_{oqrs} + \kappa_{qo}g_{pors} +\kappa_{ro}g_{pqos} + \kappa_{so}g_{pqro})
\end{align}

insert these rotated integrals into our Fock matrix,
\begin{align}
F^\kappa = h^\kappa_{pq} + 2 g^\kappa_{pqrs} D_{rs} - g^\kappa_{prqs} D_{rs}
\end{align}

expanding these equations we find,
\begin{align}
F^\kappa_{pq} \equiv {\rm\bf E}^{(2)}_{pqrs}\kappa_{rs}  =& (\kappa_{mp}h_{pn} + \kappa_{np}h_{mp}) + \notag\\
& 2( D_{rs}\kappa_{po} g_{oqrs} + \kappa_{qo} g_{pors} + \kappa_{ro} g_{pqos} + \kappa_{so} g_{pqro}) -\notag\\
&\;(D_{rs} \kappa_{po} g_{orqs} + \kappa_{qo} g_{pros} + \kappa_{ro} g_{poqs} + \kappa_{so} g_{prqo})
\end{align}

and then simplified by considering the permutational symmetry of the $g$ tensor and collapse of several elements to form various Fock matrices,
\begin{eqnarray}
{\rm\bf E}^{(2)}_{pqrs}\kappa_{rs} = ({F}_{qs}\kappa_{rs} + {F}_{pr}\kappa_{rs}) + \kappa_{rs}(4 g_{pqrs} - g_{prqs} - g_{sqrp})
\end{eqnarray}

For RHF only the occupied - virtual rotations change the Wavefunction. We can therefore simplify this above once more,
\begin{eqnarray}
{\rm\bf E}^{(2)}_{iajb}\kappa_{jb} = (-{F}_{ab}\kappa_{jb} + {F}_{ij}\kappa_{jb}) + \kappa_{jb}(4 g_{iajb} - g_{ijab} - g_{bjai})
\end{eqnarray}

Our Eqn. 1 therefore becomes,
\begin{eqnarray}
\label{final_soscf}
{\rm\bf E}^{(2)}_{iajb}\kappa_{jb} = F_{ab}
\end{eqnarray}

The easiest way to solve this is by direct inversion,
\begin{eqnarray}
\kappa_{jb} = F_{ab} [ {\rm\bf E}^{(2)}_{iajb} ] ^ {-1}
\end{eqnarray}

Solving Eqn.~\ref{final_soscf} either through direct inversion or through an iterative method is known as second-order SCF (SOSCF).
Returning to our "physicist's water molecule", the convergence pattern for SOSCF is demonstrated in Fig.~\ref{rhf_water_soscf}.
We can observe that three regular DIIS steps were required before all elements of the gradient were less than one.
The RHF wavefunction is then converged to within machine precision in terms of energy and density within 4 SOSCF iterations.
SOSCF is quite beneficial in terms of the number of iterations, but is not currently competitive in terms of cost.
 However, when solved exactly, the cost of inverting the full orbital Hessian is $o^3v^3$, unless $o$ is very small, we have effectively changed the cost of SCF from N$^4$ to N$^6$, or roughly the cost of CCSD.

\begin{figure}[htbp]
\begin{center}
\caption{A RHF computation of "physicist's water molecule" starting with a core Hamiltonian guess in the cc-pVDZ basis set utilizing DIIS and SOSCF convergence. For the SOSCF step, the Eq.~\ref{somcscf_master} has been solved exactly by inverting the full orbital Hessian. The right most abbreviations indicate the type of step taken in each iteration.}
\label{rhf_water_soscf}
{\footnotesize\linespread{1}\normalfont\ttfamily
\begin{verbatim}
RHF Iteration   1: Energy = -68.98003273414295   dE = -6.898E+01   dRMS = 1.165E-01
RHF Iteration   2: Energy = -69.64725442845806   dE = -6.672E-01   dRMS = 1.074E-01 DIIS
RHF Iteration   3: Energy = -75.79192914624532   dE = -6.144E+00   dRMS = 2.892E-02 DIIS
RHF Iteration   4: Energy = -75.97218922804181   dE = -1.802E-01   dRMS = 7.564E-03 DIIS
RHF Iteration   5: Energy = -75.98973929215161   dE = -1.755E-02   dRMS = 3.049E-04 SOSCF
RHF Iteration   6: Energy = -75.98979578473095   dE = -5.649E-05   dRMS = 1.231E-06 SOSCF
RHF Iteration   7: Energy = -75.98979578551825   dE = -7.873E-10   dRMS = 1.901E-11 SOSCF
RHF Iteration   8: Energy = -75.98979578551835   dE = -9.948E-14   dRMS = 2.395E-15 SOSCF
\end{verbatim}}
\end{center}
\end{figure}


To reduce the cost we can solve Eqn.~\ref{final_soscf} through iterative means and to avoid the costly 4-index ERI transformation we can solve it in the AO basis.
Consider the left hand side of Eqn.~\ref{final_soscf} again,
\begin{eqnarray}
\label{soscf_iter}
{F}^{\kappa}_{ia}\equiv {\rm\bf E}^{(2)}_{iajb}\kappa_{jb} = (-{F}_{ab}\kappa_{jb} + {F}_{ij}\kappa_{jb}) + \kappa_{jb}(4 g_{iajb} - g_{ijab} - g_{bjai})
\end{eqnarray}

If we are careful we can compute this product using the conventional AO-based $J$ and $K$ routines:
\begin{align}
\label{IFk_AO}
\vartheta _{\lambda\sigma} =& C_{j\lambda}\kappa_{jb}C_{b\sigma}\\
{F}^{\kappa}_{ia} =& ({F}_{ij}\kappa_{ja} - \kappa_{jb}{F}_{ba}) \notag\\&+ C_{i\mu}
(4 J[\vartheta_{\lambda\sigma}]_{\mu\nu} - K[\vartheta_{\lambda\sigma}]_{\mu\nu} - K[\vartheta_{\lambda\sigma}]_{\nu\mu})C_{a\nu}
\label{IFk_AO}
\end{align}

The cost of solving Eq.~\ref{final_soscf} is now an iterative N$^4$th when the left hand side is computed from Eq.~\ref{IFk_AO} each of these contractions (often called a microiteration) is equivalent to the cost of a normal RHF iteration.
The term macroiteration refers to a step in which the overall SCF energy is computed, this step collects all Fock builds, microiterations, and orbital rotations.

\begin{figure}[htbp]
\begin{center}
\caption{A RHF computation of "physicist's water molecule" starting with a core Hamiltonian guess in the cc-pVDZ basis set utilizing DIIS and SOSCF convergence. For the SOSCF step, Eq.~\ref{final_soscf} has been solved iteratively through a conjugate gradient method limited to four microiterations. The rightmost abbreviations indicate the type of step taken on each iteration.}
\label{rhf_water_soscf_iter}
{\footnotesize\linespread{1}\normalfont\ttfamily
\begin{verbatim}
RHF Iteration   1: Energy = -68.98003273414295   dE = -6.898E+01   dRMS = 1.165E-01
RHF Iteration   2: Energy = -69.64725442845806   dE = -6.672E-01   dRMS = 1.074E-01 DIIS
RHF Iteration   3: Energy = -75.79192914624532   dE = -6.144E+00   dRMS = 2.892E-02 DIIS
RHF Iteration   4: Energy = -75.97218922804181   dE = -1.802E-01   dRMS = 7.564E-03 DIIS
RHF Iteration   5: Energy = -75.98970327666461   dE = -1.751E-02   dRMS = 5.908E-04 SOSCF
RHF Iteration   6: Energy = -75.98979576713703   dE = -9.249E-05   dRMS = 8.777E-06 SOSCF
RHF Iteration   7: Energy = -75.98979578513478   dE = -1.799E-08   dRMS = 8.237E-07 SOSCF
RHF Iteration   8: Energy = -75.98979578551806   dE = -3.832E-10   dRMS = 1.648E-08 SOSCF
\end{verbatim}}
\end{center}
\end{figure}

Our "physicist's water molecule" was again optimized using iterative SOSCF in Fig.~\ref{rhf_water_soscf_iter}.
We observe that while the convergence is significantly faster than for the DIIS acceleration alone, the overall convergence is much slower than when Eq.~\ref{final_soscf} was solved exactly.
This is due to the fact that the number of microiterations used is insufficient to solve Eq.~\ref{final_soscf} exactly.
More microiterations can be utilized, but a balance between macro- and microiterations must be computationally efficient.
However, it should be noted that even though each microiteration of SOSCF is equivalent to the cost of a normal RHF step, the overall computational effort expended for SOSCF is generally greater than the DIIS methods for a given convergence criterion.
SOSCF should therefore only be utilized for difficult to converge SCF cases.

\section{Dipole polarizabilities}

The above is a form of orbital response for optimizing the SCF Wavefunction.
Response can also be taken "how does my Wavefunction respond to an outside force"?
When answering such questions we obtain useful molecular properties.

First let us compute polarizabilities

\begin{verbatim}
tmp_dipoles = mints.ao_dipole()
print(2 * np.vdot(tmp_dipoles[2], D))
print(mol.nuclear_dipole()[2])
\end{verbatim}

To compute response we need gradient-like vectors C and D, in addition to the Hessian,

$\langle \langle C, D \rangle \rangle =  (C^{(1)})^T ({\rm\bf E}^{(2)})^{-1}  (D^{(1)})  $

The result is effectively the average value of property C under a perturbation on the Wavefunction by D.

\bibliographystyle{aip.bst}
\bibliography{references,konrad_references,jrncodes,mainbib-ds}

\end{document}
